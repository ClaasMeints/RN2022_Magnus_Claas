\chapter{Ergebnisse}\label{ch:Ergebnisse}
    In diesem Kapitel werden die Ergebnisse der Konzeption und der Implementierung gesammelt betrachtet. Außerdem wird die Relevanz der Konzepte für das SiGI Projekt bewertet. 

\section{Ergebnisse der Implementierung}
    Zunächst werden die Ergebnisse der Implementierung des Fragmentierungansatzes zusammengefasst und bewertet. Dabei werden die grundsätzliche Machbarkeit und der Aufwand bewertet. Anschließend wird der Einfluss einer Fragmentierung auf die Laufzeit des Modells betrachtet.  

\subsection{Möglichkeiten der Fragmentierung von Modellen}
    Während der Implementierung der fragmentierbaren Klassen in die Flatbuffers Bibliothek sind keine grundsätzlichen Fehler aufgetreten. Wird versucht, die angepassten Implementierungen der Klassen zu verwenden, um ein Modell auszuführen, zeigt sich jedoch, dass nur einige der Objekte korrekt geladen werden können. Es zeigt sich, dass in einigen Fällen die Grenzen der Fragmente nicht korrekt bestimmt werden. Aufgrund der unerwartet hohen Komplexität der Implementierung und der großen Anzahl an Fragmenten ist nicht direkt ersichtlich, welches Fragment in welchem Fall falsch erzeugt wird. So muss ein falsch erzeugtes Fragment nicht zwangsläufig direkt erkannt werden. Der einzige Unterschied zu einem korrekten Fragment sind seine Grenzen. So kann ein falsch erzeugtes Fragment dazu führen, dass ein Objekt erzeugt wird, ohne dass ein Fehler auftritt. Das so erzeugte Objekt lädt wiederum seine Komponenten, verwendet für die Bestimmung der Fragmentgrenzen allerdings die Daten auf denen es erzeugt wurde. Da diese nicht korrekt sind, werden auch die Fragmente der Komponenten falsch erzeugt. Dieser Prozess kann mehrfach unbemerkt ablaufen, bis ein Wert für die Grenze eines Fragmentes festgelegt wird, welcher offensichtlich falsch ist, da er zum Beispiel außerhalb der Datei liegt.\\ In der Ausgabe des Programms zeigt sich dieses Problem dadurch, dass irgendwann ein Fragment mit falschen Grenzen gelesen werden soll. In diesem Fall sorgen die falschen Grenzen dafür, dass die Größe des Fragments größer ist als der Arbeitsspeicher, sodass der Fragment-Cache nicht genügend Arbeitsspeicher leeren kann.\\ Da das Problem erst nach dem Laden und der Verwendung vieler Objekte bzw. Fragmente auftritt und für viele Fragmente gezeigt werden kann, dass diese korrekt geladen wurden, ist davon auszugehen, dass der Fehler nur in einem bestimmten Fall auftritt. Es gibt keine Hinweise auf einen grundsätzlichen bzw. konzeptionellen Fehler. Vermutlich ist die Berechnung der Fragmentgrenzen in einer der Methoden unter bestimmten Bedingungen falsch.\\ Der Bearbeitungszeitraum dieser Arbeit hat nicht ausgereicht, um den Fehler weiter einzugrenzen oder zu beheben. Das ist unter anderem in der hohen Komplexität begründet. Die Vielzahl der Fragmente macht es kaum möglich einzugrenzen, bei welchem Fragment der Fehler initial aufgetreten ist bzw. welche Fehler nur aus der Fortsetzung anderer Fehler entstanden sind. Hätte die Zeit ausgereicht, wäre die Entwicklung eines Testprogramms zur Ausgabe der Objektpositionen im Tensorflow Lite Format sinnvoll gewesen. Damit könnten die Fragmentgrenzen abgeglichen werden und es wäre möglich, den initialen Fehler zu finden. Dazu wäre es notwendig gewesen, das Modell auf einem Gerät mit genügend Arbeitsspeicher vollständig zu laden. Geeignet wäre zum Beispiel eine Implementierung auf einem Desktop Rechner.\\ Die Ausführung des Modells mit Hilfe der neuen Klassen hat zwar nicht funktioniert, es gibt aber keine Hinweise auf eine grundsätzliche Unmöglichkeit. Vielmehr deuten die Tests auf einen einfachen Implementierungsfehler hin. Damit ist es wahrscheinlich, dass es grundsätzlich möglich ist, ein Tensorflow Modell auch dann auszuführen, wenn es größer als der Arbeitsspeicher ist. Natürlich gibt es theoretische Grenzen für die Fragmentierung des Modells, da der Interpreter zum Beispiel in seinen Anforderungen an Arbeitsspeicher unverändert bleibt.\\ Es ist allerdings auch zu beachten, dass der Programmcode durch das nachträgliche Verändern der Klassen sehr schwer wartbar und sehr fehleranfällig ist. Um eine Fragmentierung des Modells sicher zu implementieren wäre es sinnvoll, eine Bibliothek wie die Flatbuffers Bibliothek von vornherein so zu entwerfen, dass die Fragmentierung von Objekten unterstützt wird. So könnten beim Schreiben der Datei die Informationen über die Grenzen der einzelnen Objekte separat aufgelistet werden, sodass eine zweite Datei erzeugt wird, welche die zur Fragmentierung nötigen Informationen enthält.

\subsection{Laufzeitveränderung des Modells}
    Da die unterschiedlichen Komponenten des Modells während der Ausführung durch den Interpreter immer wieder von der Datei in den Arbeitsspeicher geladen werden müssen, wird die Ausführung des Modells zwangsläufig langsamer. Dieser Effekt konnte bei der erfolgreichen einstufigen Fragmentierung des Testmodells beobachtet werden. Die Verzögerungen lassen sich bei mehrfacher Ausführung des Modells zwar durch Caching reduzieren, sobald das Modell allerdings größer als der Arbeitsspeicher ist, wird es mindestens zwei Komponenten des Modells geben, welche immer wieder von der Datei geladen werden müssen. Das Konzept ähnelt damit sehr dem Swapping, welches zum Beispiel bei Desktop Rechnern angewendet wird, um zu kleinen Arbeitsspeicher zu kompensieren.\\ Grundsätzlich kann beobachtet werden, dass die Laufzeitverlängerung größer ausfällt, je größer das Modell gegenüber dem zur Verfügung stehenden Arbeitsspeicher ist.\\ Während der Tests ist außerdem aufgefallen, dass einige der Objekte immer wieder verwendet werden, während ander Teile des Modells nur sehr selten vom Interpreter benötigt werden. Durch die Verwendung eines effizienten Caching Algorithmus ließe sich also die Laufzeitverlängerung stark reduzieren. Es ist denkbar, den Caching Algorithmus speziell auf das Verhalten bzw. die Struktur eines einzelnen Modells anzupassen, um die Laufzeitverlängerung auf ein Minimum zu bringen.\\ Der Ansatz, ein Modell zu fragmentieren, beschränkt sich vor allem auf nicht zeitkritische Anwendungen. Wenn die hohe Laufzeit für den Anwendungsfall kein Problem darstellt, ist die Fragmentierung des Modells durchaus eine Möglichkeit, den Anspruch an Arbeitsspeicher und damit potenziell auch Hardwarekosten zu senken. Auch für einen Einsatz für Modelle, die eine hohe Präzision erreichen sollen, ist die Fragmentierung ein mögliches Hilfsmittel. Dadurch, dass die Anforderungen an Arbeitsspeicher eines Modells gesenkt werden können, kann in einigen Fällen möglicherweise auf den Einsatz der im Bereich der eingebetteten Systeme sehr üblichen Quantisierungsverfahren verzichtet werden. Dadurch ließen sich auch auf Mikrocontrollern potenziell Modelle mit 32-Bit Gewichten ausführen. 

\section{Bewertung der Konzepte}\label{sec:Bewertung}
    In diesem Kapitel werden die im Kapitel~\nameref{ch:Konzeption} vorgestellten Konzepte einmal abschließend bewertet.\\ Die Optimierung des Modells mit Verfahren des Tensorflow Lite Frameworks zeigt keine genügende Komprimierung, um das Modell auf einem Mikrocontroller mit \SI{4}{MiB} Arbeitsspeicher ausführen zu können. Die Quantisierung des Modells sorgt allerdings für eine deutliche Reduktion der Größe des Modells. Die Laufzeit wird dabei leicht reduziert. Der Fehler steigt leicht an. Eine Implementierung weiterer Verfahren zur Optimierung wäre ein sehr großer Aufwand und im Rahmen des SiGI Projektes nicht sinnvoll.\\ Auch die fehlerfreie  Fragmentierung des Modells wäre mit einem sehr großen Aufwand verbunden. Dieser Aufwand wurde zu Beginn der Arbeit deutlich geringer eingeschätzt. Es ist allerdings durchaus denkbar, dass die Fragmentierung von Modellen für einige Anwendungsfälle sehr lohnenswert und sinnvoll wäre. Falls es zum Beispiel zukünftig gewünscht ist, noch größere und komplexer Modelle auf Mikrocontrollern ausführen zu können, wäre die Fragmentierung vermutlich ein hilfreiches Verfahren. Allein für das Projekt SiGI wäre der Implementierungs- und Wartungsaufwand eindeutig zu hoch.\\ Ein Wechsel der Modell-Architektur ist dagegen eine durchaus valide Option. Dabei sollte aber die Tensorflow Object Detection API weiter verwendet werden. Der Aufwand für die Implementierung einer solchen Lösung wäre im Rahmen des SiGI Projektes durchaus zu bewältigen. Das Tensorflow Lite for Microcontrollers Framework könnte ohne weitere Anpassungen verwendet werden. ADie Auswirkungen eines veränderten Modells auf die Laufzeit und die Häufigkeit von Fehlern sind allerdings ungewiss. Rückblickend zeigt sich, dass dieses Konzept besser zur Lösung des Problems geeignet ist als die Fragmentierung des Modells. Ein stärkerer Fokus auf diese Lösung und eine Prüfung des Konzeptes wären sinnvoller gewesen als die Implementierung eines Fragmentierungsalgorithmus.\\ Ein Wechsel der Hardware benötigt vermutlich den geringsten Aufwand von allen vorgestellten Lösungsansätzen. Ein ESP32-S3-EYE Modul ist in der Lage, das quantisierte SiGI Modell auszuführen. Auch hier sind keine Anpassungen des Tensorflow Lite for Microcontrollers Frameworks notwendig.