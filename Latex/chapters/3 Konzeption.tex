\chapter{Konzeption und Vorgehen }\label{ch:Konzeption}
    Um die Hardwarekosten zu minimieren, ist in dieser Arbeit zu prüfen, ob der SiGI-Algorithmus auch auf einem Mikrocontroller ausgeführt werden kann. Dabei ist vor allem die Ausführung des Objekterkennungsmodells kritisch. Diese Modelle sind, wie bereits beschrieben, relativ groß im Vergleich zu anderen Neuronalen Netzen, wie Klassifikationsmodellen, und benötigen viel linear adressierbaren Arbeitsspeicher.\\ Im Rahmen des Projekts wurde im Vorfeld der Arbeit ein auf einem Klassifikationsmodell basierender Ansatz geprüft. Es existieren nicht kommerziell vertriebene Geräte auf Basis eines ESP32 und einer Kamera, welche in der Lage sind, analoge Wasserzähler abzulesen. Dabei muss der Installateur bzw. der Benutzer dem Gerät präzise mitteilen, an welchen Positionen im Bild Ziffern zu erkennen sind. So kann ein einfaches Klassifikationsmodell anstatt eines Objekterkennungsmodells verwendet werden. Aufgrund des höheren Installationsaufwands und der Anfälligkeit gegenüber leichten Bewegungen der Kamera, wurde dieser Ansatz bereits vor dieser Arbeit durch das Projektteam verworfen.\\ Um SiGI auf einem Mikrocontroller ausführen zu können, muss der Algorithmus portiert werden. Die Aufnahme von Bildern sowie die Kommunikation mit einem Backend zählen zu den Standardaufgaben, welche durch eine Reihe von Mikrocontrollern problemlos bewältigt werden können. Diese Aufgaben können außerdem mit recht wenig Rechenleistung und Arbeitsspeicher erfüllt werden. Von Interesse ist also, inwieweit sich die einzelnen Schritte der Bildverarbeitung auf einem Mikrocontroller ausführen lassen. Kritische Ressourcen sind dabei sowohl die Rechenleistung als auch der Arbeitsspeicher der Hardware. Besonders die Implementierung eines Objekterkennungsmodells auf einem Mikrocontroller ist neu. Bisher gibt es keine Berichte über die erfolgreiche Ausführung eines Objekterkennungsmodells auf einem Mikrocontroller. Die Ausführung des Modells ist aber zur Detektion der Ziffern zwingend notwendig.\\ Dieses Kapitel beschreibt in der korrekten zeitlichen Abfolge das Vorgehen bzw. die Entwicklung von Konzepten zur Lösung der Problemstellung. Im Rahmen dieser Projektarbeit erfolgt die Entwicklung und Prüfung unterschiedlicher Konzepte. Die Entwicklung neuer Konzepte baut teilweise auf den Erfahrungen vorangegangener Lösungsversuche auf. Um sich dem Problem zu nähern, wird zunächst eine Lösung auf einem ESP32-Cam Modul geprüft. Dieses Modul bietet den Vorteil, dass sowohl eine Wifi-Schnittstelle als auch eine Kamera mit ausgeführt sind und ohne großen Aufwand verwendet werden können. Das Modul verfügt über \SI{4}{MiB} linear adressierbaren Arbeitsspeicher. Das Objekterkennungsmodell hat eine Größe von \SI{12,5}{MiB}.

\section{Optimierung des Models} 
    Um das Objekterkennungsmodell auf dem ESP32-Cam ausführen zu können, muss dieses, wie bereits beschrieben, im Arbeitsspeicher abgelegt werden können. In diesem Kapitel wird zunächst untersucht, welche Verfahren zur Komprimierung des Modells eingesetzt werden können.\\ Um das Modell auf einem Mikrocontroller ausführen zu können, wird dieses zunächst ins Tensorflow Lite Format konvertiert. Dieses Format stellt das Modell als Objekte in einer Datei dar. Während dieser Konvertierung können die im Kapitel~\nameref{subsec:Laufzeit- und Speicheroptimierungsverfahren} beschriebenen Optimierungsverfahren angewendet werden.\\ Da es sich bei dem Modell um ein Neuronales Netz handelt, welches nicht mit der Keras Bibliothek entwickelt wurde, können die Optimierungsverfahren, welche nur für Keras Modelle implementiert sind, nicht genutzt werden. Es gibt grundsätzlich die Möglichkeit, diese Verfahren auch manuell durchzuführen bzw. selbst zu implementieren. Der Aufwand für die Implementierung dieser Verfahren wird allerdings als sehr hoch eingeschätzt. Auch die fehlerfreie Optimierung könnte durch fehlerhafte Implementierung eines der Optimierungsverfahren gefährdet sein. Eine Anwendung dieser Verfahren wird im Rahmen dieser Arbeit nicht in Betracht gezogen. Damit können sowohl das Beschneiden, als auch die Gruppierung von Gewichten nicht zum Einsatz kommen. Die Anzahl der zu speichernden Gewichte muss also konstant bleiben.\\ Die einzige Möglichkeit das Modell zu verkleinern ist die Quantisierung der Gewichte. Eine Quantisierung ist für alle Modelle innerhalb des Tensorflow Frameworks ohne größeren Aufwand möglich. Es gibt mehrere mögliche Vorgehensweisen und Zieltypen für die Quantisierung der Parameter. Dabei ist zu beachten, dass bei geringer Auflösung des Zieltyps auch die Präzision geringer ist, mit der das Neuronale Netz Objekte erkennen kann. Die Quantisierung führt immer zu einer Vergrößerung des Fehlers bzw. zu häufigeren Fehldetektionen.\\ Um eine möglichst starke Reduktion der Größe des Modells im Arbeitsspeicher zu erwirken, werden die Gewichte mit dem Zieltyp 8-Bit Ganzzahl quantisiert. Die Größe des Modells beträgt nach der Quantisierung noch \SI{4,4}{MiB}. Das entspricht einer Reduktion um ca. \SI{64}{\%}. Laut Dokumentation ist eine Reduktion um bis zu \SI{75}{\%} möglich. Diese wird in der Praxis nicht erreicht, da das Modell nicht nur aus Gewichten, sondern auch aus nicht quantisierbaren Komponenten besteht.\\ Mit allen anderen Zieltypen für die Quantisierung werden entweder eine gleiche oder eine geringere Reduktion der Größe erreicht. Auch eine Quantisierung vor dem Trainingsprozess bringt keine weitere Reduktion. Dieses Verfahren kann zwar dazu angewendet werden, den Fehler durch die Quantisierung zu verringern, führt aber nicht zu einer weiteren Komprimierung des Modells. Da es zunächst nur darum geht, zu zeigen ob das Modell auf einem Mikrocontroller lauffähig ist, wird das Modell nach dem Training quantisiert.\\ Das so auf \SI{4,4}{MiB} Größe reduzierte Modell muss aus dem Tensorflow Lite Format in ein für den Mikrocontroller nutzbares Format gebracht werden. Dazu werden die Binärdaten des Tensorflow Lite Formats zunächst als eine Liste von zweistelligen Hexadezimalzahlen dargestellt. Diese können dann innerhalb der Firmware des ESP32 zur Definition eines Char-Arrays genutzt werden. Da sich die Menge der Daten durch diesen Prozess nicht verändert bzw. da keine weitere Komprimierung vorgenommen wird, würde das Array im Arbeitsspeicher eine Größe von \SI{4,4}{MiB} belegen. Da die größte linear adressierte Sektion im Arbeitsspeicher des ESP32 \SI{4}{MiB} groß ist, kann kein einzelnes Array größer als \SI{4}{MiB} sein.\\ Tensorflow Lite for Microcontrollers kann das Modell nur aus einem einzelnen Char-Array erzeugen, welches alle Daten des Modells enthält. Die Verwendung des Modells ist in dieser Form auf einem ESP32 nicht möglich. Eine weitere Optimierung des bestehenden Modells ist auf Grund des hohen Implementierungsaufwands für weitere Optimierungsverfahren nicht sinnvoll.

\section{Prüfung anderer Modelle}
    Da sich das aktuell im Projekt verwendete Modell nicht stark genug komprimieren lässt, wird geprüft, ob die gleiche Aufgabe möglicherweise mit anderen Modellen erfüllt werden kann. Dabei werden zunächst die anderen Modelle der Tensorflow Object Detection API geprüft. Anschließend wird die Entwicklung eines eigenen Modells ohne API betrachtet.

\subsection{Andere Modelle der Tensorflow Object Detection API}
    Innerhalb der Tensorflow Object Detection API gibt es zurzeit vier unterschiedliche vortrainierte Neuronale Netze, welche auf der SSD MobileNet Architektur basieren, der Architektur auf der auch das SiGI Modell basiert. Die nicht trainierten Neuronalen Netze werden in das Tensorflow Lite Format konvertiert. Da sich die Anzahl der Gewichte beim Trainingsprozess nicht verändert entspricht die Größe, der vor dem Training konvertierten Neuronalen Netze, der Größe die diese auch mit Training hätten. Während der Konvertierung werden die Netze wieder quantisiert. Da auch die Quantisierung von den konkreten Werten der Gewichte unabhängig arbeitet, entspricht die Größe der untrainierten quantisierten Neuronalen Netze der Größe von trainierten Neuronalen Netzen. So kann die tatsächliche Größe der unterschiedlichen Modelle mit relativ geringem Aufwand abgeschätzt werden.\\ Innerhalb der Gruppe von Neuronalen Netzen, die auf der SSD MobileNet Architektur basieren, ist das in SiGI verwendete Modell das kleinste. Die Größen der Modelle dieser Gruppe sind mit 32-Bit Gewichten und mit 8-Bit Gewichten in Abbildung~\ref{fig:3.1} dargestellt. Das SiGI-Modell hat die Bezeichnung \textit{ssd\_mobilenet\_v2\_fpnlite\_320x320\_coco17\_tpu-8}.\fig{3.1}{Modellgrößen vor und nach der Quantisierung (Eigendarstellung)}\\ Anders sieht es bei den Modellen mit anderer Architektur aus. So kann eines der Neuronalen Netze, welches auf der CenterNet Architektur basiert, auf \SI{2,9}{MiB} komprimiert werden (siehe Abbildung~\ref{fig:3.1}). Damit wäre es grundsätzlich möglich, das Modell als ein Char-Array im Speicher abzulegen. Allerdings ist die Architektur des Neuronalen Netzes deutlich anders als die des ursprünglichen SiGI-Modells. Das Modell müsste neu trainiert werden. Auch ist unklar, inwieweit diese Modell-Architektur für den Anwendungsfall geeignet ist. Es ist nicht klar, welche Auswirkungen die veränderte Architektur auf die Präzision bzw. die Fehldetektionen des Modells hat. Auch wie sich die Laufzeit entwickelt, müsste genauer untersucht werden.\\ Ein Wechsel der Architektur ist also durchaus möglich. Andere Architekturen innerhalb der Tensorflow Object Detection API bieten durchaus die Chance, die Ziffern mit geringeren Anforderungen an Arbeitsspeicher zu detektierten. Im Vorfeld müsste der Aufwand allerdings genauer abgeschätzt werden. Möglicherweise müssen Abstriche bei Laufzeit und Präzision gemacht werden. 
    
\subsection{Prüfung einer Entwicklung ohne API}
    Auch die Entwicklung eines Modells ohne die Tensorflow Object Detection API wäre theoretisch denkbar. Dies würde den Vorteil bieten, dass die Architektur des Neuronalen Netzes flexibel an die Anforderungen von SiGI angepasst werden könnte. So könnte die Abwägung zwischen Präzision und Größe optimal getroffen werden. Zusätzlich würde sich die Möglichkeit bieten, das Modell auf Basis der Keras Bibliothek zu entwickeln. So könnten die mit der Tensorflow Object Detection API nicht nutzbaren Optimierungsverfahren eingesetzt werden. Dies hätte eine höhere Präzision bei gleichbleibender Größe zur Folge.\\ Praktisch ist der Aufwand, welcher zum Entwurf und zum Training eines anwendungsspezifischen Objektdetektormodells erforderlich wäre, zu hoch. Bekannte und erprobte Architekturen müssten angepasst werden und durch Komponenten der Keras Bibliothek nachgebaut werden. Die Entwicklung einer solchen Architektur ist enorm aufwendig.\\ Darüber hinaus würde für eine Eigenentwicklung kein vortrainiertes Modell zur Verfügung stehen. Somit müsste auch ein sehr viel größerer Aufwand beim Trainingsprozess des Neuronalen Netzes aufgebracht werden. Die Datensätze, mit denen die Modelle der Tensorflow Object Detection API Trainiert wurden, sind zwar frei verfügbar, die Kosten für die Rechenzeit des Trainingsprozess müssten allerdings voll durch das Projekt gedeckt werden.

\section{Prüfung anderer Hardware}
    Da das Modell nicht ausreichend optimiert werden kann und ein Wechsel der Architektur ein hohes Risiko darstellt, wird in diesem Kapitel geprüft, inwiefern Hardware mit mehr linear adressierbarem Arbeitsspeicher verfügbar ist. Ausgangspunkt für die Betrachtungen stellen die Erfahrungen mit dem ESP32-Cam Modul dar.

\subsection{Verschiedene Baureihen und Serien}
    Wird die Größe der Arbeitsspeicher von Mikrocontrollern gängiger Hersteller wie ST oder Microchip betrachtet, fällt auf, dass es kaum Chips bzw. Module mit Arbeitsspeicher in der Größenordnung der ESP32-Baureihe gibt. Da es im Rahmen dieser Arbeit nicht angedacht ist, eine eigene Platine zu entwerfen, sind nur Untersuchungen auf bereits entwickelten Modulen möglich. Innerhalb der ESP32-Baureihe gibt es einer Reihe verschiedener Module, welche bereits externen Arbeitsspeicher integriert haben. Für Chips anderer Hersteller ist die Auswahl deutlich geringer. Die ESP32-Baureihe ist in Bezug auf die Größe des Arbeitsspeichers praktisch alternativlos.\\ Darüber hinaus müssen auch Anforderungen an den Festwertspeicher erfüllt werden. Das Modell muss entweder aus dem Flashspeicher oder von externer Peripherie wie einer SD-Karte in den Arbeitsspeicher geladen werden. Auch hier sind einige der Module der ESP32-Baureihe gut aufgestellt.\\ In Bezug auf den linear adressierbaren Arbeitsspeicher wäre die ESP32-S3 Serie optimal geeignet. Grundsätzlich ist auch die Möglichkeit zur Adressierung externen Flashspeichers ausreichend gegeben. Da die ESP32-S3 erst seit 2020 verfügbar ist, ist die Anzahl der verfügbaren Module mit externer Beschaltung noch sehr gering. Ein Modul der ESP32-S3 Serie mit mehr als \SI{4}{MiB} linear adressierbaren Arbeitsspeicher kommt erst kurz vor Ende des Bearbeitungszeitraums dieser Arbeit auf den Markt.\\ Die ESP32 Serie ist schon seit 2016 verfügbar. Entsprechend werden auch schon zu Beginn des Bearbeitungszeitraums viele Module angeboten. Das ESP32-Cam Modul ist auch Bestandteil dieser Serie.\\ Innerhalb der Produktreihen von Espressif gibt es unterschiedliche Module mit \SI{8}{MiB} Arbeitsspeicher, zum einen das ESP32-LyraT-Mini der ESP32 Serie, zum anderen das ESP32-S3-EYE der ESP32-S3 Serie. Beide Module verfügen über eine SD-Karte sowie \SI{8}{MiB} externen Arbeitsspeicher. Damit sind beide Module grundsätzlich in der Lage, das Modell sowohl dauerhaft zu speichern als auch vollständig in den Arbeitsspeicher zu laden. Während das ESP32-S3-EYE auch einen \SI{8}{MiB} Flashspeicher hat und somit das Modell aus diesem in den Arbeitsspeicher laden kann, verfügt das ESP32-LyraT-Mini nur über \SI{4}{MiB} Flashspeicher. Das Modell muss also von der SD-Karte in den Arbeitsspeicher geladen werden.\\ Wie bereits im Kapitel~\nameref{subsec:Aufbau des Arbeitsspeichers} beschrieben, können bei Modulen der ESP32 Serie wie dem ESP32-LyraT-Mini nur maximal \SI{4}{MiB} externer Arbeitsspeicher gleichzeitig adressiert werden. Für unterschiedliche physische Speicherbereiche werden gleiche Adressen verwendet. Es kann also kein Char-Array definiert werden, welches die gesamten Daten des Modells enthält. Um den Arbeitsspeicher dennoch nutzen zu können, müsste die Tensorflow Bibliothek so angepasst werden, dass das Modell auch ausgeführt werden kann, wenn nicht das gesamte Modell als ein Char-Array gespeichert ist.\\ Anders sieht es bei dem ESP32-S3-EYE aus. Dieses Modul ist Teil der ESP32-S3 Serie. Die gesamten auf dem Modul enthaltenen \SI{8}{MiB} Arbeitsspeicher können gleichzeitig in den Adressraum abgebildet werden. Damit sollte eine problemlose Verwendung des SiGI-Modells mit dem Tensorflow Lite for Microcontrollers Framework möglich sein. Da des ESP32-S3-EYE Modul noch nicht auf dem Markt ist, kann im Rahmen dieser Arbeit keine Untersuchung mit diesem Modul erfolgen. Alle weiteren Untersuchungen und Implementierungen werden also entweder auf dem ESP32-LyraT-Mini oder dem ESp32-Cam Modul stattfinden.

\subsection{Fragmentierung des Modells}
    Um auch ohne die Verfügbarkeit des ESP32-S3-EYE Moduls die Machbarkeit einer Portierung des SiGI Algorithmus auf einen Mikrocontroller zeigen zu können, muss das Modell auf dem ESP32-LyraT-Mini ausführbar sein. Im weiteren Verlauf wird geprüft, ob es die Möglichkeit gibt, die verwendeten Bibliotheken so anzupassen, dass das Modell auch dann ausgeführt werden kann, wenn nur ein Teil der Daten gleichzeitig zur Verfügung steht. Die Idee beruht auf der Beobachtung, dass sich das Modell klar in mehrere Komponenten unterteilen lässt. Das Char-Array stellt im Speicher abgelegt die exakte Repräsentation eines Modell-Objektes dar. Dieses Modell-Objekt ist aus verschiedenen Objekten zusammengesetzt, welche wiederum nicht atomar sind.\\ Das Modell wird, wie in Kapitel~\nameref{subsec:Laufzeit- und Speicheroptimierungsverfahren} beschrieben, von einem Interpreter ausgeführt. Dieser greift über die Modell-Klasse auf die Objekte zu, aus denen das Modell aufgebaut ist. Die Analyse des Programmcodes des Interpreters zeigt, dass die einzelnen Objekte grundsätzlich voneinander unabhängig benutzt werden können. Für keine durchzuführenden Operationen ist es notwendig, dass die gesamten Daten des Modells auf einmal im Arbeitsspeicher vorhanden sind. Die meisten Methoden benötigen nur kleine Teile des Modells. Je spezifischer die Methoden und Funktionen werden, desto kleiner ist die Datenmenge, die vorausgesetzt wird. Da dem Interpreter nur ein Zeiger auf das Modell übergeben wird, wird auch an keiner Stelle das gesamte Modell kopiert.\\ Eine Fragmentierung des Modells in seine Einzelkomponenten kann, je nach Fragmentierungsgrad, die benötigte Größe an linear adressiertem Arbeitsspeicher stark reduzieren. Theoretisch wäre es denkbar, das Modell und dessen Ausführung auf atomare Typen bzw. Operationen auf atomaren Typen herunterzubrechen. Allerdings ist zu beachten, dass eine kleinteiligere Fragmentierung zu einem höheren Implementierungsaufwand führt. 

\section{Subclassing der Modell Klasse}\label{subsec:Subclassing}
    Der erste Ansatz zur Entwicklung einer fragmentierbaren Version des Modells ist die Implementierung einer eigenen Modell-Klasse, welche von der in Tensorflow Lite for Microcontrollers implementierten Modell-Klasse erbt. Die Idee hinter diesem Ansatz ist es, möglichst wenige Änderungen am Tensorflow Programmcode zu machen, um die Anzahl möglicher Fehlerquellen zu minimieren. Der Interpreter muss so nicht angepasst werden, da die Signaturen und die Struktur des Modells gleich bleiben und nur die interne Logik angepasst wird.

\subsection{Konzept Subclassing}
    Die selbst implementierte Modell-Klasse wird nicht, wie die ursprüngliche Modell-Klasse, durch einen reinterpret\_cast erzeugt, eine Typkonvertierung, bei der der Compiler die Repräsentation der Daten im Speicher nicht anpasst und diese als einen anderen Typ interpretiert. Um das Modell-Objekt überhaupt erzeugen zu können, müssen alle Klassen in der Vererbungshierarchie so angepasst werden, dass alle Konstruktoren public oder protected sind. Das so erzeugte Modell-Objekt enthält abgesehen von Zeigern auf Implementierungen virtueller Methoden und einiger Zeiger auf Daten keine eigenen Klassen-Member. Da die Daten des Modells noch nicht im Arbeitsspeicher sind, sind diese im Gegensatz zur ursprünglichen Implementierung auch nicht innerhalb des Modell-Objektes. Die Zeiger auf die Daten sind bedeutungslos.\\ Damit die Klasse durch den Interpreter benutzt werden kann, müssen alle vom Interpreter verwendeten Methoden, welche mit den Daten des Modells interagieren, überschrieben werden. Die Methoden müssen so arbeiten, dass die benötigten Daten des Modells von der SD-Karte in den Arbeitsspeicher geladen werden. Die eigentliche Funktionalität der Methoden muss dann so implementiert werden, dass sie mit den zuvor geladenen Daten korrekt arbeitet. Dazu müssen alle überschriebenen Methoden der ursprünglichen Modell-Klasse virtuell sein.\\ Da die Methoden nicht wie in der ursprünglichen Implementierung mit den Daten innerhalb der Modell-Klasse arbeiten, sondern die Daten separat geladen werden, kommt es trotz des Fehlens der Daten des Modells zu keinen unzulässigen Speicherzugriffen. So kann das Modell-Objekt, welches normalerweise größer als der Arbeitsspeicher ist, dennoch verwendet werden.\\ Damit die nicht weiter zu fragmentierenden Objekte dem Interpreter korrekt übergeben werden können, müssen diese zunächst komplett von der SD-Karte geladen werden. Auf der betreffenden Sektion des Arbeitsspeichers ist dann ein reinterpret\_cast zum jeweiligen Zielobjekt durchzuführen. Die so erzeugten Objekte können dann wie gewohnt verwendet werden.\\ Auf diese Weise kann das Modell in seine Bestandteile zerlegt werden. Die einzelnen Objekte werden separat von der SD-Karte geladen. Da das Laden von der SD-Karte deutlich langsamer ist als der normale Zugriff auf den Arbeitsspeicher, wird ein einfacher Caching-Algorithmus zur Optimierung verwendet. Dabei werden die Objekte zunächst nach der Benutzung im Arbeitsspeicher belassen. Reicht beim Laden eines neuen Objekts der Arbeitsspeicher nicht aus, wird der Reihe nach immer das größte Objekt gelöscht, bis der Arbeitsspeicher ausreicht.\\ Mit den vorgenommenen Änderungen lässt sich ein kleineres Test-Modell bereits erfolgreich ausführen. Die Komponenten werden wie erwartet einzeln geladen und der Interpreter gibt für Testdaten die korrekten Ergebnisse aus. Damit ist gezeigt, dass das Konzept grundsätzlich funktioniert. Bei der Ausführung des SiGI-Modells fällt allerdings auf, dass einer der Komponenten allein größer ist als der linear adressierbare Arbeitsspeicher. Entsprechend kann das Programm nicht ausreichend Speicher für die größte Komponente allokieren. Um dieses Problem zu umgehen, muss eine zweistufige Fragmentierung implementiert werden.

\subsection{Anpassungen von Klassen der Flatbuffers Bibliothek}
    Für die Speicherung und das Laden von Objekten wird innerhalb des Tensorflow Lite for Microcontrollers Frameworks die Flatbuffers Bibliothek verwendet. Diese bietet eine Reihe von Klassen, deren Objekte im Tensorflow Lite Format in eine Datei geschrieben werden können. Das Format ist so gewählt, dass die Daten im Arbeitsspeicher wieder als Objekte verwendet werden können. Die Klassen bilden die Grundlage für das Modell und alle darin enthaltenen Objekte. Die Klassen jedes dieser Objekte erben von mindestens einer der Flatbuffers Klassen oder können direkt aus den Daten erzeugt werden. Alle Komponenten, in die das Modell in der ersten Fragmentierung zerlegt wurde, sind Objekte von Klassen, welche von Flatbuffers Klassen erben.\\ Um  nun eine zweistufige Fragmentierung implementieren zu können, müssen also auch diese Klassen angepasst werden. Das Objekt, welches nach der einstufigen Fragmentierung noch zu groß ist, ist ein Flatbuffers Vektor. Dieser bildet innerhalb der Flatbuffers Bibliothek die wesentlichen Funktionen des Vektor der Standardbibliothek ab. Um auch den Vektor in seine einzelnen Elemente zerlegen zu können bzw. um ihn auch verwenden zu können, wenn nicht alle Elemente in den Arbeitsspeicher geladen wurden, muss nur eine einzige Methode angepasst werden. Diese Methode hat die Aufgabe, ein Element anhand des Indexes zurückzugeben. Um die Methode anpassen zu können, wird diese wieder als virtuell deklariert.\\ Statt der Flatbuffers Vektor-Klasse wir dann eine selbst implementierte Vektor-Klasse verwendet, welche von der Flatbuffers Vektor-Klasse erbt. Die Implementierung der zu überschreibenden Methode wird so angepasst, dass zunächst die Größe und die Position des jeweiligen Elements bestimmt wird. Anschließend wird dieser Bereich in den Arbeitsspeicher geladen. Auf den Daten im Arbeitsspeicher wird ein reinterpret\_cast zum Zieltyp ausgeführt.\\ Beim Testen der Implementierung kommt es zu Fehlern beim Laden unterschiedlicher Objekte. Es zeigt sich, dass andere Objekte, welche auf der Flatbuffers Vektor-Klasse basieren, nicht mehr auf die gleiche Weise wie vor den Anpassungen geladen werden. Bei den nicht fragmentierten Vektoren kommt es beim Zugriff auf die einzelnen Elemente zu Verschiebungen. Die Elemente sind nicht korrekt gelesen und somit nicht verwendbar. 

\subsection{Veränderte Speicherrepräsentation von virtuellen Methoden}
    Die Ursache für die Verschiebung muss mit der Änderung der Flatbuffers Vektor-Klasse zusammenhängen, da alle anderen Teile des Programmcodes unverändert sind. Dadurch, dass die Flatbuffers Vektor-Klasse nun eine virtuelle Methode enthält, muss sie auch die Information enthalten, wo die konkrete Implementierung der Methode zu finden ist. Der C++-Compiler speichert diese Information in Form eines Zeigers auf die entsprechende Methodenimplementierung. Dieser Zeiger wird oberhalb der Klassen-Member erwartet. Da alle verwendeten ESP32 Module einen 32-Bit Adressraum haben, werden vier Bytes vom Compiler als Adresse verwendet.\\ Wird nun ein Vektor aus den gespeicherten Daten durch einen reinterpret\_cast erzeugt, werden die ersten vier Bytes der Vektor-Daten als Zeiger interpretiert. Sollen nun die Elemente des Vektors zurückgegeben werden, wird der Beginn der Elemente jeweils um vier Bytes im Speicher verschoben. Die so erzeugten Elemente des Vektor sind somit nicht korrekt befüllt.\\ Das Problem ist, dass die Daten in der Datei unter Anwendung der nicht veränderten Flatbuffers Vektor-Klasse erzeugt wurden. Entsprechend sind die Zeiger auf die Methodenimplementierung nicht vorhanden. Durch die Virtualisierung ändert sich die Repräsentation der Klasse im Arbeitsspeicher. Um also Objekte, welche von Flatbuffers Klassen erben, weiterhin durch einen reinterpret\_cast der Daten im Tensorflow Lite Format erzeugen zu können, darf die Klasse nur so angepasst werden, dass die Repräsentation der Objekte im Arbeitsspeicher nicht verändert wird. 

\section{Überschreiben der Implementierung der Flatbuffers Klassen}
    Um die Probleme, welche durch die Implementierung virtueller Methoden entstanden sind, zu beheben, wird nun versucht, die Tensorflow Lite for Microcontrollers und die Flatbuffers Klassen direkt zu verändern. Ziel ist es, die Klassen so anzupassen, dass sie unverändert verwendet werden können und die Repräsentation der Objekte im Arbeitsspeicher gleich bleibt.

\subsection{Konzeption}
    Die größte Veränderung im Vergleich zu dem im Kapitel~\nameref{subsec:Subclassing} beschriebenen Ansatz ist, dass die eigenen Implementierungen nicht in neue Klassen geschrieben werden, welche von den ursprünglichen Klassen erben, sondern dass die ursprünglichen Klassen überschrieben werden. Dadurch stehen die ursprünglichen Klassen dem Programm nicht mehr zur Verfügung. Dies vergrößert die Komplexität der Aufgabe, da die Klassen nun sowohl fragmentiert als auch nicht fragmentiert benutzbar sein müssen. Innerhalb der Methoden muss die Implementierung für beide Varianten sowie die Information darüber, welche der Varianten auszuführen ist, bekannt sein.\\ Eine weitere wichtige Einschränkung ist, dass weder die Signaturen noch die Repräsentation der Klassen im Arbeitsspeicher angepasst werden können. Bei veränderten Signaturen kann der Interpreter die Klassen nicht mehr wie gewohnt verwenden. Bei einer Änderung der Repräsentation im Arbeitsspeicher kommt es zu dem bereits beschriebenen Problem, dass die Daten des Modells nicht mehr korrekt zu Objekten der betreffenden Klassen gecastet werden können.\\ Um dennoch in der Lage zu sein, sowohl Hilfsmethoden als auch neue Klassen-Member zu verwenden, wird jeder zu verändernden Klasse eine Hilfsklasse angeheftet. Objekte dieser Hilfsklassen sind immer einem Objekt der ursprünglichen Klasse zugeordnet.\\ Zusätzlich ist jedem Objekt, welches aus der Datei in den Arbeitsspeicher geladen wird, ein sogenanntes Fragment-Objekt zugeordnet. Fragmente stellen eine Schnittstelle zum Laden und Speichern von Objekten. Sie enthalten die Informationen über die Größe und Position der zu ladeneden Objekte und können zentral verwaltet werden. Sie sorgen auch dafür, dass, wenn ein Objekt auf Daten außerhalb des eigenen Fragments zugreifen will, das richtige andere Fragment gegebenenfalls erzeugt und geladen wird.\\ So gibt es für das Modell ein Hauptfragment, welches die Enden der Datei mit den Modelldaten als Grenzen hat. Soll nun eine Komponente des Modells verwendet werden, kann innerhalb der Grenzen des Hauptfragment eine neues Fragment für das zu ladende Objekt geschaffen werden. Die Fragmentgrenzen werden also immer durch die Klasse bestimmt, welche das Objekt beinhaltet bzw. welche dieses mit einer Methode zurück gibt. Es bildet sich für eine mehrstufige Fragmentierung also ein verschachteltes System von Fragmenten, die jeweils einem Objekt zugeordnet werden. Nur die Fragmente auf der letzten Ebene werden tatsächlich aus der Datei in den Arbeitsspeicher geladen. Alle darüber liegenden Fragmente stellen nur den Rahmen ihrer fragmentierten Objekte.

\subsection{Implementierung}
    Um jedem Objekt einer fragmentierbaren Klasse wie beschrieben ein Objekt der Hilfsklasse zuzuordnen, wird die Adresse des jeweiligen Objekts als Referenz verwendet. Die Hilfsklasse hat eine statische Methode, welche eine statische Variable enthält. Diese Variable ist vom Typ Map. Sie ordnet Adressen jeweils Objekte der Hilfsklasse zu. Jedes Objekt kann so über die eigene Adresse die zu ihm gehörende Instanz der Hilfsklasse bekommen und verwenden (vgl. \nameref{sec:Hilfsklasse}).\\ Die Hilfsklasse speichert unter anderem die den Objekten zugehörigen Fragmente. So kann ein Objekt jederzeit das ihm zugeordnete Fragment ansprechen, um zum Beispiel zu prüfen, ob ein Speicherzugriff innerhalb oder außerhalb des Fragments liegt. Das ist immer dann notwendig, wenn ein Objekt nicht alle Daten, auf die zugegriffen werden muss, selbst enthält. Dadurch, dass jedes fragmentierte Objekt auch ein Fragment hat, welches den ihm zugeordneten Speicherbereich abgrenzt, kann unterschieden werden, ob auf bereits geladenen oder noch nicht geladene Daten zugegriffen wird (vgl. \nameref{sec:Fragment}).\\ Soll auf ein Objekt zugegriffen werden, erfolgt dies immer über das jeweilige Fragment. Dieses lädt die Daten aus der Datei, falls diese nicht mehr oder noch nicht geladen sind, und gibt dann einen Zeiger auf das Objekt zurück. Intern wird mit einem Smart-Pointer gearbeitet. Soll ein Objekt gelöscht werden, um Platz im Arbeitsspeicher zu schaffen, wird der Smart-Pointer des zugehörigen Fragments auf den Nullpointer gesetzt. Dadurch wird die einzige Kopie des Smart-Pointers auf die Daten des Objekts gelöscht, womit der zugehörige Arbeitsspeicher automatisch freigegeben wird.\\ Da ein Objekt immer nur Zugriff auf die Objekte hat, aus denen es besteht, muss es zum Löschen von Objekten eine zentrale Stelle geben, welche Zugriff auf alle Fragmente ermöglicht. Diese Aufgabe wird von der Fragment-Cache Klasse übernommen. Jedem Objekt ist mit Hilfe der Hilfsklasse eine Instanz des Caches zugeordnet. Diese ordnet Indices jeweils einer Komponente des Objektes zu. So ist im Cache eines Vektors in einer Map jedem Index ein Fragment zugeordnet. Soll der Vektor nun ein Element an einem Index zurückgeben, wird das entsprechende Fragment durch den Cache gefunden und zurückgegeben. Wurde das Objekt seit dem letzten Zugriff nicht gelöscht, ist der Zeiger innerhalb des Fragments noch valide und das Objekt kann ohne erneutes Laden aus der Datei verwendet werden.\\ Tatsächlich erfolgt innerhalb des Fragment-Caches keine direkte Zuordnung der Indices zu den Objekten. Damit der Fragment-Cache als zentrale Stelle mit Zugriff auf alle Fragmente dienen kann, sind diese in einer Map jeweils einem globalen Index zugeordnet. Diese Map ist eine statische Klassenvariable. Alle Instanzen des Fragment-Caches verwenden also die gleiche globale Indexierung und die gleiche Map. Damit es nicht zu falschen Zugriffen kommt, weil zum Beispiel zwei Vektoren jeweils ein Element mit dem Index null haben, verfügt jede Instanz des Fragment-Caches über einen Klassen-Member, eine Map, welche lokale Indices den globalen Indices zugeordnet. Soll ein neuer globaler Index einem neuen Fragment zugeordnet werden, wird eine statische Variable inkrementiert. Der neue Wert dieser Variable wird dann sowohl dem Fragment als auch dem lokalen Index zugeordnet(vgl. \nameref{sec:Cache}).\\ Soll nun Arbeitsspeicher geleert werden, wählt der Fragment-Cache Fragmente aus, welche gelöscht werden sollen. Aktuell wird das jeweils größte Fragment sooft gelöscht, bis der Arbeitsspeicher ausreicht. Um die Effizienz des Fragment-Caches zu erhöhen, könnte dieses Verfahren so angepasst werden, dass möglichst selten ein Fragment aus der Datei geladen werden muss.